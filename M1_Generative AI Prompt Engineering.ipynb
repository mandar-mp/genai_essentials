{"cells":[{"cell_type":"markdown","id":"wP-5EROrF-QF","metadata":{"id":"wP-5EROrF-QF"},"source":["# Gen AI Essentials\n","Module-1 Generative AI prompt engineering"]},{"cell_type":"markdown","id":"DAtaol7DFMgy","metadata":{"id":"DAtaol7DFMgy"},"source":["\n","In this notebook, we will explore what a **prompt** is, understand the concept of **prompt engineering**, and learn how to interact with a **large language model (LLM)** using **Google Gemini**. This tutorial includes practical examples relevant to enterprise use cases."]},{"cell_type":"markdown","id":"AzkljZmdFMg0","metadata":{"id":"AzkljZmdFMg0"},"source":["### 1. What is a Prompt?\n","A **prompt** is the input provided to a generative AI model to produce a desired output. It typically includes specific instructions, questions, or context to guide the model in generating the required response.\n","**Examples:**\n","- `Summarize this document: <text>`\n","- `Generate a Python function to calculate factorial.`\n","Prompts can range from simple queries to complex instructions depending on the task."]},{"cell_type":"markdown","id":"uN-qr4jUFMg0","metadata":{"id":"uN-qr4jUFMg0"},"source":["### 2. What is Prompt Engineering?\n","Prompt engineering is the process of designing and refining prompts to improve the quality and relevance of responses generated by an AI model. Effective prompts often:\n","- Include clear instructions\n","- Provide necessary context\n","- Are formatted correctly (e.g., using specific keywords or delimiters)\n","\n","**Example of a poorly designed prompt:**\n","`Explain this topic.`\n","**Improved prompt:**\n","`Explain the importance of data privacy regulations in cloud computing.`"]},{"cell_type":"markdown","id":"WyucNPAQFMg0","metadata":{"id":"WyucNPAQFMg0"},"source":["### 3. Why is Prompt Engineering Important?\n","Prompt engineering is critical for the following reasons:\n","- **Accuracy:** A well-crafted prompt reduces ambiguity and improves response accuracy.\n","- **Efficiency:** Saves time by avoiding repeated trial and error.\n","- **Customization:** Aligns the model's output with specific business needs.\n","- **Cost-effectiveness:** Minimizes computational costs by reducing unnecessary iterations."]},{"cell_type":"markdown","id":"AcM9xXqRFMg0","metadata":{"id":"AcM9xXqRFMg0"},"source":["### 4. Connecting to Google Gemini model using LangChain\n","Below is a code example demonstrating how to connect to Google Gemini using the LangChain library. Ensure you have your Google Gemini API key ready."]},{"cell_type":"code","execution_count":1,"id":"q6S4dcl25qkw","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q6S4dcl25qkw","executionInfo":{"status":"ok","timestamp":1736519582646,"user_tz":-330,"elapsed":61788,"user":{"displayName":"Mandar Patil","userId":"04941921403953627260"}},"outputId":"18ef203a-db74-4e65-a338-65acab2c74fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/mandar_gdrive\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/mandar_gdrive')"]},{"cell_type":"code","execution_count":null,"id":"N9yVuk1J2UTn","metadata":{"id":"N9yVuk1J2UTn"},"outputs":[],"source":["# Install langchain and related dependencies\n","# !pip install langchain\n","# !pip install langchain_community\n","!pip install langchain-google-genai"]},{"cell_type":"code","execution_count":6,"id":"O29R-3tB4zQT","metadata":{"id":"O29R-3tB4zQT","executionInfo":{"status":"ok","timestamp":1736521582739,"user_tz":-330,"elapsed":1228,"user":{"displayName":"Mandar Patil","userId":"04941921403953627260"}}},"outputs":[],"source":["import yaml\n","with open('/content/mandar_gdrive/MyDrive/Trainings/GenAI Essentials/config.yml', 'r') as f:\n","    config = yaml.load(f,Loader=yaml.SafeLoader )"]},{"cell_type":"code","execution_count":7,"id":"4GuloRYFFMg1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":656,"status":"ok","timestamp":1736521587019,"user":{"displayName":"Mandar Patil","userId":"04941921403953627260"},"user_tz":-330},"id":"4GuloRYFFMg1","outputId":"f19bbcb4-87e3-4dd3-9439-f0e37c696747"},"outputs":[{"output_type":"stream","name":"stdout","text":["------------------------------------------\n","Why don't scientists trust atoms? \n","\n","Because they make up everything!\n"]}],"source":["from langchain_google_genai import ChatGoogleGenerativeAI\n","\n","# Create a Gemini LLM instance\n","# For Gemini 1.5 pro use # model=\"gemini-1.5-pro\"\n","llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", google_api_key =config['GEMINI_API_KEY'] )\n","\n","# Prompt the LLM\n","prompt = \"Tell me a joke\"\n","response = llm.invoke(prompt)\n","print(\"------------------------------------------\")\n","print(response.content)"]},{"cell_type":"markdown","id":"ia-K8GqKFMg1","metadata":{"id":"ia-K8GqKFMg1"},"source":["### 5. Simple Interaction with LLM\n","Once connected, you can send a message to the model and receive a response. Let's try an example."]},{"cell_type":"code","execution_count":8,"id":"_GPhQq-XFMg1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1372,"status":"ok","timestamp":1736521806524,"user":{"displayName":"Mandar Patil","userId":"04941921403953627260"},"user_tz":-330},"id":"_GPhQq-XFMg1","outputId":"8b0a97af-f3d0-4ac6-e519-1a2fee3f2d6c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"J'adore programmer.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}],"source":["messages = [\n","    (\"system\", \"Translate the user sentence to French.\"),\n","    (\"human\", \"I love programming.\"),\n","]\n","result = llm.invoke(messages)\n","result.content"]},{"cell_type":"markdown","id":"nrOMUIdhFMg1","metadata":{"id":"nrOMUIdhFMg1"},"source":["### 6. Understanding LLM Parameters\n","LLMs have several tunable parameters that influence their responses:\n","- **Temperature:** Controls randomness in the output. Higher values result in more creative responses.\n","\n","\n","**Examples:**"]},{"cell_type":"code","execution_count":9,"id":"_A3IVwI_FMg1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2053,"status":"ok","timestamp":1736521844899,"user":{"displayName":"Mandar Patil","userId":"04941921403953627260"},"user_tz":-330},"id":"_A3IVwI_FMg1","outputId":"a2010dd4-922a-4b7c-b33b-26f458c855e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Temperature set to 1 to create more creative responses\n","Temperature 1 Response: I went to the farmer's market and bought a bushel of fresh peaches.\n","Temperature 1 Response: I went to the farmer's market and bought a basket overflowing with fresh produce.\n"]}],"source":["# Example: Temperature parameter\n","temp = 1\n","gemini_llm1 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",\n","                                    google_api_key =config['GEMINI_API_KEY'],\n","                                    temperature=temp)\n","\n","print(\"Temperature set to 1 to create more creative responses\")\n","for response_retry in range(0,2):\n","  response = gemini_llm1.invoke(\"Write a sentence starting with 'I went to...'\")\n","\n","  print(f\"Temperature {temp} Response:\", response.content)"]},{"cell_type":"code","execution_count":11,"id":"sPt6pJeAdedx","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2683,"status":"ok","timestamp":1736521900011,"user":{"displayName":"Mandar Patil","userId":"04941921403953627260"},"user_tz":-330},"id":"sPt6pJeAdedx","outputId":"77b8d478-0671-4b50-906d-59029bc99b2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Temperature set to 0 to create more creative responses\n","Temperature 0.0 Response: I went to the farmer's market and bought a bushel of ripe peaches.\n","Temperature 0.0 Response: I went to the farmer's market and bought a bushel of ripe peaches.\n","Temperature 0.0 Response: I went to the farmer's market and bought a bushel of ripe peaches.\n"]}],"source":["# Example: Temperature parameter\n","temp = 0.0\n","gemini_llm2 = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\",\n","                                    google_api_key =config['GEMINI_API_KEY'],\n","                                    temperature=temp)\n","print(\"Temperature set to 0 to create more creative responses\")\n","for response_retry in range(0,3):\n","  response = gemini_llm2.invoke(\"Write a sentence starting with 'I went to...'\")\n","\n","  print(f\"Temperature {temp} Response:\", response.content)"]},{"cell_type":"markdown","id":"QThmZN7QFMg1","metadata":{"id":"QThmZN7QFMg1"},"source":["### Conclusion\n","By mastering prompt engineering and understanding LLM parameters, developers can unlock the full potential of generative AI for enterprise applications. Experiment with different prompts and parameters to tailor the model's responses to your specific use case."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}